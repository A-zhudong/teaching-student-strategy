nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21916934 parameters

Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 156, in <module>
    model = get_model(data_dir, mat_prop, classification, \
  File "train_crabnet_finetune.py", line 33, in get_model
    model.load_network(transfer, finetune=True)
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 537, in load_network
    pretrained_state_dict = {k: v for k, v in network['weights'] \
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 537, in <dictcomp>
    pretrained_state_dict = {k: v for k, v in network['weights'] \
ValueError: too many values to unpack (expected 2)
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21916934 parameters

Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 156, in <module>
    model = get_model(data_dir, mat_prop, classification, \
  File "train_crabnet_finetune.py", line 33, in get_model
    model.load_network(transfer, finetune=True)
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 537, in load_network
    pretrained_state_dict = {k: v for k, v in network['weights'] if not k.startswith('decoder') and not k.startswith('transformer_decoder')}
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 537, in <dictcomp>
    pretrained_state_dict = {k: v for k, v in network['weights'] if not k.startswith('decoder') and not k.startswith('transformer_decoder')}
ValueError: too many values to unpack (expected 2)
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21916934 parameters

<class 'collections.OrderedDict'>
Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 156, in <module>
    model = get_model(data_dir, mat_prop, classification, \
  File "train_crabnet_finetune.py", line 33, in get_model
    model.load_network(transfer, finetune=True)
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 538, in load_network
    pretrained_state_dict = {k: v for k, v in network['weights'] if not k.startswith('decoder') and not k.startswith('transformer_decoder')}
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 538, in <dictcomp>
    pretrained_state_dict = {k: v for k, v in network['weights'] if not k.startswith('decoder') and not k.startswith('transformer_decoder')}
ValueError: too many values to unpack (expected 2)
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21916934 parameters

<class 'collections.OrderedDict'>
loaded modules: dict_keys(['encoder.emb_scaler', 'encoder.pos_scaler', 'encoder.pos_scaler_log', 'encoder.embed.fc_mat2vec.weight', 'encoder.embed.fc_mat2vec.bias', 'encoder.embed.cbfv.weight', 'encoder.pe.pe', 'encoder.ple.pe', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.0.linear1.weight', 'encoder.transformer_encoder.layers.0.linear1.bias', 'encoder.transformer_encoder.layers.0.linear2.weight', 'encoder.transformer_encoder.layers.0.linear2.bias', 'encoder.transformer_encoder.layers.0.norm1.weight', 'encoder.transformer_encoder.layers.0.norm1.bias', 'encoder.transformer_encoder.layers.0.norm2.weight', 'encoder.transformer_encoder.layers.0.norm2.bias', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.1.linear1.weight', 'encoder.transformer_encoder.layers.1.linear1.bias', 'encoder.transformer_encoder.layers.1.linear2.weight', 'encoder.transformer_encoder.layers.1.linear2.bias', 'encoder.transformer_encoder.layers.1.norm1.weight', 'encoder.transformer_encoder.layers.1.norm1.bias', 'encoder.transformer_encoder.layers.1.norm2.weight', 'encoder.transformer_encoder.layers.1.norm2.bias', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.2.linear1.weight', 'encoder.transformer_encoder.layers.2.linear1.bias', 'encoder.transformer_encoder.layers.2.linear2.weight', 'encoder.transformer_encoder.layers.2.linear2.bias', 'encoder.transformer_encoder.layers.2.norm1.weight', 'encoder.transformer_encoder.layers.2.norm1.bias', 'encoder.transformer_encoder.layers.2.norm2.weight', 'encoder.transformer_encoder.layers.2.norm2.bias', 'output_nn.fcs.0.weight', 'output_nn.fcs.0.bias', 'output_nn.fcs.1.weight', 'output_nn.fcs.1.bias', 'output_nn.fcs.2.weight', 'output_nn.fcs.2.bias', 'output_nn.res_fcs.0.weight', 'output_nn.res_fcs.1.weight', 'output_nn.res_fcs.2.weight', 'output_nn.fc_out.weight', 'output_nn.fc_out.bias', 'res_nn.fcs.0.weight', 'res_nn.fcs.0.bias', 'res_nn.fcs.1.weight', 'res_nn.fcs.1.bias', 'res_nn.fcs.2.weight', 'res_nn.fcs.2.bias', 'res_nn.fc_out.weight', 'res_nn.fc_out.bias', 'src_nn.weight', 'src_nn.bias', 'final.weight', 'final.bias', 'final_trans.weight', 'final_trans.bias', 'input.weight', 'input.bias', 'output.weight', 'output.bias', 'encoder_layer.self_attn.in_proj_weight', 'encoder_layer.self_attn.in_proj_bias', 'encoder_layer.self_attn.out_proj.weight', 'encoder_layer.self_attn.out_proj.bias', 'encoder_layer.linear1.weight', 'encoder_layer.linear1.bias', 'encoder_layer.linear2.weight', 'encoder_layer.linear2.bias', 'encoder_layer.norm1.weight', 'encoder_layer.norm1.bias', 'encoder_layer.norm2.weight', 'encoder_layer.norm2.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias'])
Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  32%|███▏      | 9085/28639 [00:00<00:00, 90838.39formulae/s]Generating EDM:  71%|███████▏  | 20439/28639 [00:00<00:00, 104184.62formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 104514.92formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 162273.71formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/300 --- train mae: 0.654 val mae: 0.632
Epoch: 1/300 --- train mae: 0.65 val mae: 0.627
Epoch: 3/300 --- train mae: 0.65 val mae: 0.627
Epoch: 5/300 --- train mae: 0.65 val mae: 0.627
Epoch: 7/300 --- train mae: 0.65 val mae: 0.628
Epoch: 9/300 --- train mae: 0.651 val mae: 0.628
Epoch: 11/300 --- train mae: 0.65 val mae: 0.628
Epoch: 13/300 --- train mae: 0.65 val mae: 0.628
Epoch: 15/300 --- train mae: 0.65 val mae: 0.627
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917190 parameters

<class 'collections.OrderedDict'>
loaded modules: dict_keys(['encoder.emb_scaler', 'encoder.pos_scaler', 'encoder.pos_scaler_log', 'encoder.embed.fc_mat2vec.weight', 'encoder.embed.fc_mat2vec.bias', 'encoder.embed.cbfv.weight', 'encoder.pe.pe', 'encoder.ple.pe', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.0.linear1.weight', 'encoder.transformer_encoder.layers.0.linear1.bias', 'encoder.transformer_encoder.layers.0.linear2.weight', 'encoder.transformer_encoder.layers.0.linear2.bias', 'encoder.transformer_encoder.layers.0.norm1.weight', 'encoder.transformer_encoder.layers.0.norm1.bias', 'encoder.transformer_encoder.layers.0.norm2.weight', 'encoder.transformer_encoder.layers.0.norm2.bias', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.1.linear1.weight', 'encoder.transformer_encoder.layers.1.linear1.bias', 'encoder.transformer_encoder.layers.1.linear2.weight', 'encoder.transformer_encoder.layers.1.linear2.bias', 'encoder.transformer_encoder.layers.1.norm1.weight', 'encoder.transformer_encoder.layers.1.norm1.bias', 'encoder.transformer_encoder.layers.1.norm2.weight', 'encoder.transformer_encoder.layers.1.norm2.bias', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.2.linear1.weight', 'encoder.transformer_encoder.layers.2.linear1.bias', 'encoder.transformer_encoder.layers.2.linear2.weight', 'encoder.transformer_encoder.layers.2.linear2.bias', 'encoder.transformer_encoder.layers.2.norm1.weight', 'encoder.transformer_encoder.layers.2.norm1.bias', 'encoder.transformer_encoder.layers.2.norm2.weight', 'encoder.transformer_encoder.layers.2.norm2.bias', 'output_nn.fcs.0.weight', 'output_nn.fcs.0.bias', 'output_nn.fcs.1.weight', 'output_nn.fcs.1.bias', 'output_nn.fcs.2.weight', 'output_nn.fcs.2.bias', 'output_nn.res_fcs.0.weight', 'output_nn.res_fcs.1.weight', 'output_nn.res_fcs.2.weight', 'output_nn.fc_out.weight', 'output_nn.fc_out.bias', 'res_nn.fcs.0.weight', 'res_nn.fcs.0.bias', 'res_nn.fcs.1.weight', 'res_nn.fcs.1.bias', 'res_nn.fcs.2.weight', 'res_nn.fcs.2.bias', 'res_nn.fc_out.weight', 'res_nn.fc_out.bias', 'src_nn.weight', 'src_nn.bias', 'final.weight', 'final.bias', 'final_trans.weight', 'final_trans.bias', 'input.weight', 'input.bias', 'output.weight', 'output.bias', 'encoder_layer.self_attn.in_proj_weight', 'encoder_layer.self_attn.in_proj_bias', 'encoder_layer.self_attn.out_proj.weight', 'encoder_layer.self_attn.out_proj.bias', 'encoder_layer.linear1.weight', 'encoder_layer.linear1.bias', 'encoder_layer.linear2.weight', 'encoder_layer.linear2.bias', 'encoder_layer.norm1.weight', 'encoder_layer.norm1.bias', 'encoder_layer.norm2.weight', 'encoder_layer.norm2.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias'])
Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  37%|███▋      | 10734/28639 [00:00<00:00, 107326.03formulae/s]Generating EDM:  79%|███████▉  | 22595/28639 [00:00<00:00, 113954.90formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 113867.74formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 153592.47formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/300 --- train mae: 0.693 val mae: 0.67
Epoch: 1/300 --- train mae: 0.65 val mae: 0.628
Epoch: 3/300 --- train mae: 0.651 val mae: 0.629
Epoch: 5/300 --- train mae: 0.65 val mae: 0.627
Epoch: 7/300 --- train mae: 0.651 val mae: 0.628
Epoch: 9/300 --- train mae: 0.651 val mae: 0.628
Epoch: 11/300 --- train mae: 0.65 val mae: 0.627
Epoch: 13/300 --- train mae: 0.65 val mae: 0.627
Epoch: 15/300 --- train mae: 0.65 val mae: 0.627
Epoch: 17/300 --- train mae: 0.65 val mae: 0.627
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917190 parameters

Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  39%|███▉      | 11292/28639 [00:00<00:00, 112906.65formulae/s]Generating EDM:  80%|████████  | 22964/28639 [00:00<00:00, 115143.94formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 115364.21formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 162299.33formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/300 --- train mae: 0.75 val mae: 0.729
Epoch: 1/300 --- train mae: 0.475 val mae: 0.46
Epoch: 3/300 --- train mae: 0.449 val mae: 0.446
Epoch: 5/300 --- train mae: 0.416 val mae: 0.417
Epoch: 7/300 --- train mae: 0.378 val mae: 0.373
Epoch: 9/300 --- train mae: 0.354 val mae: 0.355
Epoch: 11/300 --- train mae: 0.337 val mae: 0.336
Epoch: 13/300 --- train mae: 0.322 val mae: 0.326
Epoch: 15/300 --- train mae: 0.307 val mae: 0.314
Epoch: 17/300 --- train mae: 0.291 val mae: 0.298
Epoch: 19/300 --- train mae: 0.285 val mae: 0.3
Epoch: 21/300 --- train mae: 0.272 val mae: 0.288
Epoch: 23/300 --- train mae: 0.273 val mae: 0.291
Epoch: 25/300 --- train mae: 0.26 val mae: 0.279
Epoch: 27/300 --- train mae: 0.257 val mae: 0.276
Epoch: 29/300 --- train mae: 0.253 val mae: 0.274
Epoch: 31/300 --- train mae: 0.242 val mae: 0.264
Epoch: 33/300 --- train mae: 0.232 val mae: 0.261
Epoch: 35/300 --- train mae: 0.237 val mae: 0.262
Epoch 37 failed to improve.
Discarded: 1/150 weight updates ♻🗑️
Epoch: 37/300 --- train mae: 0.228 val mae: 0.266
Epoch: 39/300 --- train mae: 0.221 val mae: 0.256
Epoch: 41/300 --- train mae: 0.214 val mae: 0.252
Epoch 43 failed to improve.
Discarded: 2/150 weight updates ♻🗑️
Epoch: 43/300 --- train mae: 0.212 val mae: 0.256
Epoch: 45/300 --- train mae: 0.203 val mae: 0.251
Epoch: 47/300 --- train mae: 0.203 val mae: 0.25
Epoch: 49/300 --- train mae: 0.205 val mae: 0.245
Epoch: 51/300 --- train mae: 0.2 val mae: 0.24
Epoch: 53/300 --- train mae: 0.193 val mae: 0.235
Epoch: 55/300 --- train mae: 0.194 val mae: 0.242
Epoch 57 failed to improve.
Discarded: 3/150 weight updates ♻🗑️
Epoch: 57/300 --- train mae: 0.191 val mae: 0.24
Epoch: 59/300 --- train mae: 0.184 val mae: 0.234
Epoch 61 failed to improve.
Discarded: 4/150 weight updates ♻🗑️
Epoch: 61/300 --- train mae: 0.187 val mae: 0.241
Epoch: 63/300 --- train mae: 0.181 val mae: 0.234
Epoch: 65/300 --- train mae: 0.173 val mae: 0.228
Epoch 67 failed to improve.
Discarded: 5/150 weight updates ♻🗑️
Epoch: 67/300 --- train mae: 0.173 val mae: 0.234
Epoch 69 failed to improve.
Discarded: 6/150 weight updates ♻🗑️
Epoch: 69/300 --- train mae: 0.172 val mae: 0.233
Epoch 71 failed to improve.
Discarded: 7/150 weight updates ♻🗑️
Epoch: 71/300 --- train mae: 0.167 val mae: 0.239
Epoch 73 failed to improve.
Discarded: 8/150 weight updates ♻🗑️
Epoch: 73/300 --- train mae: 0.17 val mae: 0.234
Epoch: 75/300 --- train mae: 0.164 val mae: 0.225
Epoch 77 failed to improve.
Discarded: 9/150 weight updates ♻🗑️
Epoch: 77/300 --- train mae: 0.17 val mae: 0.23
Epoch: 79/300 --- train mae: 0.163 val mae: 0.221
Epoch 81 failed to improve.
Discarded: 10/150 weight updates ♻🗑️
Epoch: 81/300 --- train mae: 0.163 val mae: 0.224
Epoch 83 failed to improve.
Discarded: 11/150 weight updates ♻🗑️
Epoch: 83/300 --- train mae: 0.161 val mae: 0.222
Epoch 85 failed to improve.
Discarded: 12/150 weight updates ♻🗑️
Epoch: 85/300 --- train mae: 0.157 val mae: 0.222
Epoch 87 failed to improve.
Discarded: 13/150 weight updates ♻🗑️
Epoch: 87/300 --- train mae: 0.155 val mae: 0.224
Epoch: 89/300 --- train mae: 0.153 val mae: 0.219
Epoch 91 failed to improve.
Discarded: 14/150 weight updates ♻🗑️
Epoch: 91/300 --- train mae: 0.155 val mae: 0.223
Epoch 93 failed to improve.
Discarded: 15/150 weight updates ♻🗑️
Epoch: 93/300 --- train mae: 0.154 val mae: 0.221
Epoch: 95/300 --- train mae: 0.15 val mae: 0.218
Epoch: 97/300 --- train mae: 0.147 val mae: 0.215
Epoch 99 failed to improve.
Discarded: 16/150 weight updates ♻🗑️
Epoch: 99/300 --- train mae: 0.149 val mae: 0.221
Epoch: 101/300 --- train mae: 0.146 val mae: 0.217
Epoch: 103/300 --- train mae: 0.147 val mae: 0.215
Epoch: 105/300 --- train mae: 0.141 val mae: 0.216
Epoch 107 failed to improve.
Discarded: 17/150 weight updates ♻🗑️
Epoch: 107/300 --- train mae: 0.146 val mae: 0.22
Epoch 109 failed to improve.
Discarded: 18/150 weight updates ♻🗑️
Epoch: 109/300 --- train mae: 0.145 val mae: 0.223
Epoch: 111/300 --- train mae: 0.14 val mae: 0.207
Epoch: 113/300 --- train mae: 0.133 val mae: 0.207
Epoch: 115/300 --- train mae: 0.135 val mae: 0.209
Epoch: 117/300 --- train mae: 0.135 val mae: 0.208
Epoch: 119/300 --- train mae: 0.132 val mae: 0.204
Epoch: 121/300 --- train mae: 0.132 val mae: 0.204
Epoch 123 failed to improve.
Discarded: 19/150 weight updates ♻🗑️
Epoch: 123/300 --- train mae: 0.131 val mae: 0.21
Epoch 125 failed to improve.
Discarded: 20/150 weight updates ♻🗑️
Epoch: 125/300 --- train mae: 0.128 val mae: 0.209
Epoch 127 failed to improve.
Discarded: 21/150 weight updates ♻🗑️
Epoch: 127/300 --- train mae: 0.13 val mae: 0.216
Epoch 129 failed to improve.
Discarded: 22/150 weight updates ♻🗑️
Epoch: 129/300 --- train mae: 0.13 val mae: 0.216
Epoch 131 failed to improve.
Discarded: 23/150 weight updates ♻🗑️
Epoch: 131/300 --- train mae: 0.13 val mae: 0.214
Epoch 133 failed to improve.
Discarded: 24/150 weight updates ♻🗑️
Epoch: 133/300 --- train mae: 0.131 val mae: 0.211
Epoch 135 failed to improve.
Discarded: 25/150 weight updates ♻🗑️
Epoch: 135/300 --- train mae: 0.127 val mae: 0.209
Epoch: 137/300 --- train mae: 0.126 val mae: 0.204
Epoch 139 failed to improve.
Discarded: 26/150 weight updates ♻🗑️
Epoch: 139/300 --- train mae: 0.123 val mae: 0.211
Epoch 141 failed to improve.
Discarded: 27/150 weight updates ♻🗑️
Epoch: 141/300 --- train mae: 0.123 val mae: 0.208
Epoch 143 failed to improve.
Discarded: 28/150 weight updates ♻🗑️
Epoch: 143/300 --- train mae: 0.122 val mae: 0.208
Epoch 145 failed to improve.
Discarded: 29/150 weight updates ♻🗑️
Epoch: 145/300 --- train mae: 0.119 val mae: 0.208
Epoch 147 failed to improve.
Discarded: 30/150 weight updates ♻🗑️
Epoch: 147/300 --- train mae: 0.129 val mae: 0.211
Epoch 149 failed to improve.
Discarded: 31/150 weight updates ♻🗑️
Epoch: 149/300 --- train mae: 0.129 val mae: 0.209
Epoch 151 failed to improve.
Discarded: 32/150 weight updates ♻🗑️
Epoch: 151/300 --- train mae: 0.118 val mae: 0.217
Epoch 153 failed to improve.
Discarded: 33/150 weight updates ♻🗑️
Epoch: 153/300 --- train mae: 0.12 val mae: 0.21
Epoch 155 failed to improve.
Discarded: 34/150 weight updates ♻🗑️
Epoch: 155/300 --- train mae: 0.118 val mae: 0.209
Epoch 157 failed to improve.
Discarded: 35/150 weight updates ♻🗑️
Epoch: 157/300 --- train mae: 0.119 val mae: 0.207
Epoch 159 failed to improve.
Discarded: 36/150 weight updates ♻🗑️
Epoch: 159/300 --- train mae: 0.122 val mae: 0.206
Epoch 161 failed to improve.
Discarded: 37/150 weight updates ♻🗑️
Epoch: 161/300 --- train mae: 0.119 val mae: 0.208
Epoch 163 failed to improve.
Discarded: 38/150 weight updates ♻🗑️
Epoch: 163/300 --- train mae: 0.119 val mae: 0.205
Epoch: 165/300 --- train mae: 0.117 val mae: 0.205
Epoch: 167/300 --- train mae: 0.114 val mae: 0.203
Epoch 169 failed to improve.
Discarded: 39/150 weight updates ♻🗑️
Epoch: 169/300 --- train mae: 0.116 val mae: 0.205
Epoch: 171/300 --- train mae: 0.112 val mae: 0.201
Epoch 173 failed to improve.
Discarded: 40/150 weight updates ♻🗑️
Epoch: 173/300 --- train mae: 0.115 val mae: 0.204
Epoch 175 failed to improve.
Discarded: 41/150 weight updates ♻🗑️
Epoch: 175/300 --- train mae: 0.115 val mae: 0.205
Epoch 177 failed to improve.
Discarded: 42/150 weight updates ♻🗑️
Epoch: 177/300 --- train mae: 0.113 val mae: 0.206
Epoch 179 failed to improve.
Discarded: 43/150 weight updates ♻🗑️
Epoch: 179/300 --- train mae: 0.113 val mae: 0.209
Epoch 181 failed to improve.
Discarded: 44/150 weight updates ♻🗑️
Epoch: 181/300 --- train mae: 0.113 val mae: 0.204
Epoch: 183/300 --- train mae: 0.111 val mae: 0.196
Epoch 185 failed to improve.
Discarded: 45/150 weight updates ♻🗑️
Epoch: 185/300 --- train mae: 0.114 val mae: 0.209
Epoch 187 failed to improve.
Discarded: 46/150 weight updates ♻🗑️
Epoch: 187/300 --- train mae: 0.109 val mae: 0.204
Epoch 189 failed to improve.
Discarded: 47/150 weight updates ♻🗑️
Epoch: 189/300 --- train mae: 0.107 val mae: 0.202
Epoch 191 failed to improve.
Discarded: 48/150 weight updates ♻🗑️
Epoch: 191/300 --- train mae: 0.109 val mae: 0.204
Epoch 193 failed to improve.
Discarded: 49/150 weight updates ♻🗑️
Epoch: 193/300 --- train mae: 0.109 val mae: 0.209
Epoch 195 failed to improve.
Discarded: 50/150 weight updates ♻🗑️
Epoch: 195/300 --- train mae: 0.109 val mae: 0.204
Epoch 197 failed to improve.
Discarded: 51/150 weight updates ♻🗑️
Epoch: 197/300 --- train mae: 0.11 val mae: 0.201
Epoch 199 failed to improve.
Discarded: 52/150 weight updates ♻🗑️
Epoch: 199/300 --- train mae: 0.111 val mae: 0.21
Epoch 201 failed to improve.
Discarded: 53/150 weight updates ♻🗑️
Epoch: 201/300 --- train mae: 0.103 val mae: 0.207
Epoch 203 failed to improve.
Discarded: 54/150 weight updates ♻🗑️
Epoch: 203/300 --- train mae: 0.104 val mae: 0.2
Epoch 205 failed to improve.
Discarded: 55/150 weight updates ♻🗑️
Epoch: 205/300 --- train mae: 0.105 val mae: 0.21
Epoch 207 failed to improve.
Discarded: 56/150 weight updates ♻🗑️
Epoch: 207/300 --- train mae: 0.104 val mae: 0.201
Epoch 209 failed to improve.
Discarded: 57/150 weight updates ♻🗑️
Epoch: 209/300 --- train mae: 0.104 val mae: 0.208
Epoch: 211/300 --- train mae: 0.102 val mae: 0.198
Epoch 213 failed to improve.
Discarded: 58/150 weight updates ♻🗑️
Epoch: 213/300 --- train mae: 0.102 val mae: 0.209
Epoch 215 failed to improve.
Discarded: 59/150 weight updates ♻🗑️
Epoch: 215/300 --- train mae: 0.103 val mae: 0.204
Epoch: 217/300 --- train mae: 0.0987 val mae: 0.196
Epoch: 219/300 --- train mae: 0.0988 val mae: 0.194
Epoch: 221/300 --- train mae: 0.1 val mae: 0.201
Epoch: 223/300 --- train mae: 0.0997 val mae: 0.194
Epoch 225 failed to improve.
Discarded: 60/150 weight updates ♻🗑️
Epoch: 225/300 --- train mae: 0.0991 val mae: 0.201
Epoch 227 failed to improve.
Discarded: 61/150 weight updates ♻🗑️
Epoch: 227/300 --- train mae: 0.0987 val mae: 0.207
Epoch 229 failed to improve.
Discarded: 62/150 weight updates ♻🗑️
Epoch: 229/300 --- train mae: 0.0968 val mae: 0.203
Epoch: 231/300 --- train mae: 0.099 val mae: 0.197
Epoch 233 failed to improve.
Discarded: 63/150 weight updates ♻🗑️
Epoch: 233/300 --- train mae: 0.0968 val mae: 0.201
Epoch 235 failed to improve.
Discarded: 64/150 weight updates ♻🗑️
Epoch: 235/300 --- train mae: 0.1 val mae: 0.202
Epoch 237 failed to improve.
Discarded: 65/150 weight updates ♻🗑️
Epoch: 237/300 --- train mae: 0.0991 val mae: 0.202
Epoch 239 failed to improve.
Discarded: 66/150 weight updates ♻🗑️
Epoch: 239/300 --- train mae: 0.0986 val mae: 0.199
Epoch 241 failed to improve.
Discarded: 67/150 weight updates ♻🗑️
Epoch: 241/300 --- train mae: 0.0978 val mae: 0.198
Epoch 243 failed to improve.
Discarded: 68/150 weight updates ♻🗑️
Epoch: 243/300 --- train mae: 0.097 val mae: 0.196
Epoch 245 failed to improve.
Discarded: 69/150 weight updates ♻🗑️
Epoch: 245/300 --- train mae: 0.0944 val mae: 0.198
Epoch 247 failed to improve.
Discarded: 70/150 weight updates ♻🗑️
Epoch: 247/300 --- train mae: 0.0946 val mae: 0.197
Epoch: 249/300 --- train mae: 0.0948 val mae: 0.197
Epoch 251 failed to improve.
Discarded: 71/150 weight updates ♻🗑️
Epoch: 251/300 --- train mae: 0.0941 val mae: 0.206
Epoch 253 failed to improve.
Discarded: 72/150 weight updates ♻🗑️
Epoch: 253/300 --- train mae: 0.0923 val mae: 0.201
Epoch 255 failed to improve.
Discarded: 73/150 weight updates ♻🗑️
Epoch: 255/300 --- train mae: 0.0904 val mae: 0.199
Epoch 257 failed to improve.
Discarded: 74/150 weight updates ♻🗑️
Epoch: 257/300 --- train mae: 0.0945 val mae: 0.204
Epoch 259 failed to improve.
Discarded: 75/150 weight updates ♻🗑️
Epoch: 259/300 --- train mae: 0.0915 val mae: 0.208
Epoch 261 failed to improve.
Discarded: 76/150 weight updates ♻🗑️
Epoch: 261/300 --- train mae: 0.0902 val mae: 0.198
Epoch 263 failed to improve.
Discarded: 77/150 weight updates ♻🗑️
Epoch: 263/300 --- train mae: 0.0892 val mae: 0.203
Epoch 265 failed to improve.
Discarded: 78/150 weight updates ♻🗑️
Epoch: 265/300 --- train mae: 0.0869 val mae: 0.203
Epoch 267 failed to improve.
Discarded: 79/150 weight updates ♻🗑️
Epoch: 267/300 --- train mae: 0.0884 val mae: 0.199
Epoch: 269/300 --- train mae: 0.0879 val mae: 0.194
Epoch: 271/300 --- train mae: 0.0901 val mae: 0.191
Epoch 273 failed to improve.
Discarded: 80/150 weight updates ♻🗑️
Epoch: 273/300 --- train mae: 0.0882 val mae: 0.203
Epoch 275 failed to improve.
Discarded: 81/150 weight updates ♻🗑️
Epoch: 275/300 --- train mae: 0.089 val mae: 0.204
Epoch 277 failed to improve.
Discarded: 82/150 weight updates ♻🗑️
Epoch: 277/300 --- train mae: 0.0869 val mae: 0.199
Epoch 279 failed to improve.
Discarded: 83/150 weight updates ♻🗑️
Epoch: 279/300 --- train mae: 0.0883 val mae: 0.197
Epoch 281 failed to improve.
Discarded: 84/150 weight updates ♻🗑️
Epoch: 281/300 --- train mae: 0.0882 val mae: 0.204
Epoch 283 failed to improve.
Discarded: 85/150 weight updates ♻🗑️
Epoch: 283/300 --- train mae: 0.0884 val mae: 0.202
Epoch 285 failed to improve.
Discarded: 86/150 weight updates ♻🗑️
Epoch: 285/300 --- train mae: 0.0888 val mae: 0.201
Epoch 287 failed to improve.
Discarded: 87/150 weight updates ♻🗑️
Epoch: 287/300 --- train mae: 0.0875 val mae: 0.199
Epoch 289 failed to improve.
Discarded: 88/150 weight updates ♻🗑️
Epoch: 289/300 --- train mae: 0.0899 val mae: 0.202
Epoch 291 failed to improve.
Discarded: 89/150 weight updates ♻🗑️
Epoch: 291/300 --- train mae: 0.0837 val mae: 0.197
Epoch 293 failed to improve.
Discarded: 90/150 weight updates ♻🗑️
Epoch: 293/300 --- train mae: 0.0843 val mae: 0.199
Epoch 295 failed to improve.
Discarded: 91/150 weight updates ♻🗑️
Epoch: 295/300 --- train mae: 0.087 val mae: 0.203
Epoch 297 failed to improve.
Discarded: 92/150 weight updates ♻🗑️
Epoch: 297/300 --- train mae: 0.0856 val mae: 0.206
Epoch 299 failed to improve.
Discarded: 93/150 weight updates ♻🗑️
Epoch: 299/300 --- train mae: 0.0838 val mae: 0.196
Saving network (jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune) to /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer/jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune.pth
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 168, in <module>
    model_train, mae_train = save_results(data_dir, mat_prop, classification,
  File "train_crabnet_finetune.py", line 107, in save_results
    model, output = get_results(model)
  File "train_crabnet_finetune.py", line 99, in get_results
    output = model.predict(model.data_loader)  # predict the data saved here
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 485, in predict
    output, _ = self.model.forward(src, frac)
TypeError: forward() missing 1 required positional argument: 'trg'
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917190 parameters

Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  27%|██▋       | 7639/28639 [00:00<00:00, 76381.51formulae/s]Generating EDM:  62%|██████▏   | 17616/28639 [00:00<00:00, 90128.76formulae/s]Generating EDM:  95%|█████████▍| 27142/28639 [00:00<00:00, 92466.90formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 89386.27formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 137557.10formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/300 --- train mae: 0.75 val mae: 0.729
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917190 parameters

Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  25%|██▍       | 7035/28639 [00:00<00:00, 70340.34formulae/s]Generating EDM:  51%|█████▏    | 14722/28639 [00:00<00:00, 74177.40formulae/s]Generating EDM:  78%|███████▊  | 22278/28639 [00:00<00:00, 74804.08formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 76832.19formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 136844.10formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/300 --- train mae: 0.75 val mae: 0.729
nohup: ignoring input
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 169, in <module>
    model_train, mae_train = save_results(data_dir, mat_prop, classification,
  File "train_crabnet_finetune.py", line 106, in save_results
    model = load_model(data_dir, mat_prop, classification, file_name, verbose=verbose,\
  File "train_crabnet_finetune.py", line 82, in load_model
    model = Model(CrabNet(compute_device=compute_device, pretrained=pretrained).to(compute_device),\
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 120, in __init__
    with open(embedding_path, 'rb') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType
nohup: ignoring input
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 169, in <module>
    model_train, mae_train = save_results(data_dir, mat_prop, classification,
  File "train_crabnet_finetune.py", line 106, in save_results
    model = load_model(data_dir, mat_prop, classification, file_name, verbose=verbose,\
  File "train_crabnet_finetune.py", line 82, in load_model
    model = Model(CrabNet(compute_device=compute_device, pretrained=pretrained).to(compute_device),\
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 120, in __init__
    with open(embedding_path, 'rb') as f:
TypeError: expected str, bytes or os.PathLike object, not NoneType
nohup: ignoring input
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
Traceback (most recent call last):
  File "train_crabnet_finetune.py", line 169, in <module>
    model_train, mae_train = save_results(data_dir, mat_prop, classification,
  File "train_crabnet_finetune.py", line 108, in save_results
    model, output = get_results(model)
  File "train_crabnet_finetune.py", line 100, in get_results
    output = model.predict(model.data_loader)  # predict the data saved here
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 473, in predict
    trg = [self.csv_atoms[formu] for formu in formula]
  File "/hdd/nas_157/home/zd/zd/teaching_net/CrabNet/crabnet/model.py", line 473, in <listcomp>
    trg = [self.csv_atoms[formu] for formu in formula]
AttributeError: 'Model' object has no attribute 'csv_atoms'
nohup: ignoring input
  File "train_crabnet_finetune.py", line 108
    model = load_model(data_dir, mat_prop, classification, file_name, verbose=verbose,\
                        model_name=model_name, save_model_path=save_model_path,\
                        pretrained=pretrained, , embedding_path=embedding_path)
                                               ^
SyntaxError: invalid syntax
nohup: ignoring input
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.0896
-----------------------------------------------------
calculating val mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.191
-----------------------------------------------------
calculating test mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.213
=====================================================
nohup: ignoring input
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.0896
-----------------------------------------------------
calculating val mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.191
-----------------------------------------------------
calculating test mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.213
=====================================================
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917190 parameters

<class 'collections.OrderedDict'>
loaded modules: dict_keys(['cls_token', 'encoder.emb_scaler', 'encoder.pos_scaler', 'encoder.pos_scaler_log', 'encoder.embed.fc_mat2vec.weight', 'encoder.embed.fc_mat2vec.bias', 'encoder.embed.cbfv.weight', 'encoder.pe.pe', 'encoder.ple.pe', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.0.linear1.weight', 'encoder.transformer_encoder.layers.0.linear1.bias', 'encoder.transformer_encoder.layers.0.linear2.weight', 'encoder.transformer_encoder.layers.0.linear2.bias', 'encoder.transformer_encoder.layers.0.norm1.weight', 'encoder.transformer_encoder.layers.0.norm1.bias', 'encoder.transformer_encoder.layers.0.norm2.weight', 'encoder.transformer_encoder.layers.0.norm2.bias', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.1.linear1.weight', 'encoder.transformer_encoder.layers.1.linear1.bias', 'encoder.transformer_encoder.layers.1.linear2.weight', 'encoder.transformer_encoder.layers.1.linear2.bias', 'encoder.transformer_encoder.layers.1.norm1.weight', 'encoder.transformer_encoder.layers.1.norm1.bias', 'encoder.transformer_encoder.layers.1.norm2.weight', 'encoder.transformer_encoder.layers.1.norm2.bias', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.2.linear1.weight', 'encoder.transformer_encoder.layers.2.linear1.bias', 'encoder.transformer_encoder.layers.2.linear2.weight', 'encoder.transformer_encoder.layers.2.linear2.bias', 'encoder.transformer_encoder.layers.2.norm1.weight', 'encoder.transformer_encoder.layers.2.norm1.bias', 'encoder.transformer_encoder.layers.2.norm2.weight', 'encoder.transformer_encoder.layers.2.norm2.bias', 'output_nn.fcs.0.weight', 'output_nn.fcs.0.bias', 'output_nn.fcs.1.weight', 'output_nn.fcs.1.bias', 'output_nn.fcs.2.weight', 'output_nn.fcs.2.bias', 'output_nn.res_fcs.0.weight', 'output_nn.res_fcs.1.weight', 'output_nn.res_fcs.2.weight', 'output_nn.fc_out.weight', 'output_nn.fc_out.bias', 'res_nn.fcs.0.weight', 'res_nn.fcs.0.bias', 'res_nn.fcs.1.weight', 'res_nn.fcs.1.bias', 'res_nn.fcs.2.weight', 'res_nn.fcs.2.bias', 'res_nn.fc_out.weight', 'res_nn.fc_out.bias', 'src_nn.weight', 'src_nn.bias', 'final.weight', 'final.bias', 'final_trans.weight', 'final_trans.bias', 'input.weight', 'input.bias', 'output.weight', 'output.bias', 'encoder_layer.self_attn.in_proj_weight', 'encoder_layer.self_attn.in_proj_bias', 'encoder_layer.self_attn.out_proj.weight', 'encoder_layer.self_attn.out_proj.bias', 'encoder_layer.linear1.weight', 'encoder_layer.linear1.bias', 'encoder_layer.linear2.weight', 'encoder_layer.linear2.bias', 'encoder_layer.norm1.weight', 'encoder_layer.norm1.bias', 'encoder_layer.norm2.weight', 'encoder_layer.norm2.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias', 'final_res.fcs.0.weight', 'final_res.fcs.0.bias', 'final_res.fcs.1.weight', 'final_res.fcs.1.bias', 'final_res.fc_out.weight', 'final_res.fc_out.bias', 'final_final.weight', 'final_final.bias'])
Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  25%|██▌       | 7226/28639 [00:00<00:00, 72248.70formulae/s]Generating EDM:  60%|██████    | 17323/28639 [00:00<00:00, 89137.42formulae/s]Generating EDM:  95%|█████████▌| 27283/28639 [00:00<00:00, 93908.65formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 91190.77formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 139742.86formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/100 --- train mae: 0.0995 val mae: 0.212
Epoch: 1/100 --- train mae: 0.0863 val mae: 0.202
Epoch: 3/100 --- train mae: 0.0861 val mae: 0.202
Epoch: 5/100 --- train mae: 0.0937 val mae: 0.202
Epoch: 7/100 --- train mae: 0.0899 val mae: 0.2
Epoch 9 failed to improve.
Discarded: 1/150 weight updates ♻🗑️
Epoch: 9/100 --- train mae: 0.0881 val mae: 0.2
Epoch 11 failed to improve.
Discarded: 2/150 weight updates ♻🗑️
Epoch: 11/100 --- train mae: 0.0871 val mae: 0.203
Epoch: 13/100 --- train mae: 0.0871 val mae: 0.198
Epoch 15 failed to improve.
Discarded: 3/150 weight updates ♻🗑️
Epoch: 15/100 --- train mae: 0.0881 val mae: 0.202
Epoch 17 failed to improve.
Discarded: 4/150 weight updates ♻🗑️
Epoch: 17/100 --- train mae: 0.0884 val mae: 0.208
Epoch 19 failed to improve.
Discarded: 5/150 weight updates ♻🗑️
Epoch: 19/100 --- train mae: 0.0873 val mae: 0.202
Epoch: 21/100 --- train mae: 0.0873 val mae: 0.197
Epoch 23 failed to improve.
Discarded: 6/150 weight updates ♻🗑️
Epoch: 23/100 --- train mae: 0.0853 val mae: 0.203
Epoch 25 failed to improve.
Discarded: 7/150 weight updates ♻🗑️
Epoch: 25/100 --- train mae: 0.0856 val mae: 0.206
Epoch 27 failed to improve.
Discarded: 8/150 weight updates ♻🗑️
Epoch: 27/100 --- train mae: 0.0846 val mae: 0.198
Epoch: 29/100 --- train mae: 0.0853 val mae: 0.198
Epoch 31 failed to improve.
Discarded: 9/150 weight updates ♻🗑️
Epoch: 31/100 --- train mae: 0.0869 val mae: 0.203
Epoch 33 failed to improve.
Discarded: 10/150 weight updates ♻🗑️
Epoch: 33/100 --- train mae: 0.0845 val mae: 0.202
Epoch: 35/100 --- train mae: 0.0832 val mae: 0.2
Epoch: 37/100 --- train mae: 0.0832 val mae: 0.196
Epoch: 39/100 --- train mae: 0.0818 val mae: 0.198
Epoch 41 failed to improve.
Discarded: 11/150 weight updates ♻🗑️
Epoch: 41/100 --- train mae: 0.0824 val mae: 0.2
Epoch 43 failed to improve.
Discarded: 12/150 weight updates ♻🗑️
Epoch: 43/100 --- train mae: 0.0838 val mae: 0.206
Epoch 45 failed to improve.
Discarded: 13/150 weight updates ♻🗑️
Epoch: 45/100 --- train mae: 0.0834 val mae: 0.196
Epoch: 47/100 --- train mae: 0.083 val mae: 0.199
Epoch: 49/100 --- train mae: 0.0821 val mae: 0.196
Epoch 51 failed to improve.
Discarded: 14/150 weight updates ♻🗑️
Epoch: 51/100 --- train mae: 0.0817 val mae: 0.2
Epoch 53 failed to improve.
Discarded: 15/150 weight updates ♻🗑️
Epoch: 53/100 --- train mae: 0.0818 val mae: 0.202
Epoch 55 failed to improve.
Discarded: 16/150 weight updates ♻🗑️
Epoch: 55/100 --- train mae: 0.0807 val mae: 0.2
Epoch 57 failed to improve.
Discarded: 17/150 weight updates ♻🗑️
Epoch: 57/100 --- train mae: 0.0791 val mae: 0.202
Epoch 59 failed to improve.
Discarded: 18/150 weight updates ♻🗑️
Epoch: 59/100 --- train mae: 0.082 val mae: 0.205
Epoch 61 failed to improve.
Discarded: 19/150 weight updates ♻🗑️
Epoch: 61/100 --- train mae: 0.0798 val mae: 0.201
Epoch 63 failed to improve.
Discarded: 20/150 weight updates ♻🗑️
Epoch: 63/100 --- train mae: 0.0802 val mae: 0.208
Epoch 65 failed to improve.
Discarded: 21/150 weight updates ♻🗑️
Epoch: 65/100 --- train mae: 0.0818 val mae: 0.21
Epoch 67 failed to improve.
Discarded: 22/150 weight updates ♻🗑️
Epoch: 67/100 --- train mae: 0.0831 val mae: 0.208
Epoch 69 failed to improve.
Discarded: 23/150 weight updates ♻🗑️
Epoch: 69/100 --- train mae: 0.081 val mae: 0.205
Epoch 71 failed to improve.
Discarded: 24/150 weight updates ♻🗑️
Epoch: 71/100 --- train mae: 0.0801 val mae: 0.207
Epoch 73 failed to improve.
Discarded: 25/150 weight updates ♻🗑️
Epoch: 73/100 --- train mae: 0.0783 val mae: 0.204
Epoch 75 failed to improve.
Discarded: 26/150 weight updates ♻🗑️
Epoch: 75/100 --- train mae: 0.0834 val mae: 0.199
Epoch 77 failed to improve.
Discarded: 27/150 weight updates ♻🗑️
Epoch: 77/100 --- train mae: 0.0811 val mae: 0.201
Epoch 79 failed to improve.
Discarded: 28/150 weight updates ♻🗑️
Epoch: 79/100 --- train mae: 0.0807 val mae: 0.199
Epoch: 81/100 --- train mae: 0.0793 val mae: 0.197
Epoch 83 failed to improve.
Discarded: 29/150 weight updates ♻🗑️
Epoch: 83/100 --- train mae: 0.0828 val mae: 0.199
Epoch: 85/100 --- train mae: 0.0762 val mae: 0.193
Epoch 87 failed to improve.
Discarded: 30/150 weight updates ♻🗑️
Epoch: 87/100 --- train mae: 0.0796 val mae: 0.203
Epoch 89 failed to improve.
Discarded: 31/150 weight updates ♻🗑️
Epoch: 89/100 --- train mae: 0.077 val mae: 0.202
Epoch 91 failed to improve.
Discarded: 32/150 weight updates ♻🗑️
Epoch: 91/100 --- train mae: 0.0792 val mae: 0.201
Epoch 93 failed to improve.
Discarded: 33/150 weight updates ♻🗑️
Epoch: 93/100 --- train mae: 0.08 val mae: 0.204
Epoch 95 failed to improve.
Discarded: 34/150 weight updates ♻🗑️
Epoch: 95/100 --- train mae: 0.077 val mae: 0.205
Epoch 97 failed to improve.
Discarded: 35/150 weight updates ♻🗑️
Epoch: 97/100 --- train mae: 0.0772 val mae: 0.205
Epoch 99 failed to improve.
Discarded: 36/150 weight updates ♻🗑️
Epoch: 99/100 --- train mae: 0.0807 val mae: 0.207
Saving network (jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune) to /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer/jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune.pth
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.0762
-----------------------------------------------------
calculating val mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.193
-----------------------------------------------------
calculating test mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.208
=====================================================
