nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917702 parameters

<class 'collections.OrderedDict'>
loaded modules: dict_keys(['cls_token', 'encoder.emb_scaler', 'encoder.pos_scaler', 'encoder.pos_scaler_log', 'encoder.embed.fc_mat2vec.weight', 'encoder.embed.fc_mat2vec.bias', 'encoder.embed.cbfv.weight', 'encoder.pe.pe', 'encoder.ple.pe', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.0.linear1.weight', 'encoder.transformer_encoder.layers.0.linear1.bias', 'encoder.transformer_encoder.layers.0.linear2.weight', 'encoder.transformer_encoder.layers.0.linear2.bias', 'encoder.transformer_encoder.layers.0.norm1.weight', 'encoder.transformer_encoder.layers.0.norm1.bias', 'encoder.transformer_encoder.layers.0.norm2.weight', 'encoder.transformer_encoder.layers.0.norm2.bias', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.1.linear1.weight', 'encoder.transformer_encoder.layers.1.linear1.bias', 'encoder.transformer_encoder.layers.1.linear2.weight', 'encoder.transformer_encoder.layers.1.linear2.bias', 'encoder.transformer_encoder.layers.1.norm1.weight', 'encoder.transformer_encoder.layers.1.norm1.bias', 'encoder.transformer_encoder.layers.1.norm2.weight', 'encoder.transformer_encoder.layers.1.norm2.bias', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.2.linear1.weight', 'encoder.transformer_encoder.layers.2.linear1.bias', 'encoder.transformer_encoder.layers.2.linear2.weight', 'encoder.transformer_encoder.layers.2.linear2.bias', 'encoder.transformer_encoder.layers.2.norm1.weight', 'encoder.transformer_encoder.layers.2.norm1.bias', 'encoder.transformer_encoder.layers.2.norm2.weight', 'encoder.transformer_encoder.layers.2.norm2.bias', 'output_nn.fcs.0.weight', 'output_nn.fcs.0.bias', 'output_nn.fcs.1.weight', 'output_nn.fcs.1.bias', 'output_nn.fcs.2.weight', 'output_nn.fcs.2.bias', 'output_nn.res_fcs.0.weight', 'output_nn.res_fcs.1.weight', 'output_nn.res_fcs.2.weight', 'output_nn.fc_out.weight', 'output_nn.fc_out.bias', 'res_nn.fcs.0.weight', 'res_nn.fcs.0.bias', 'res_nn.fcs.1.weight', 'res_nn.fcs.1.bias', 'res_nn.fcs.2.weight', 'res_nn.fcs.2.bias', 'res_nn.fc_out.weight', 'res_nn.fc_out.bias', 'src_nn.weight', 'src_nn.bias', 'final.weight', 'final.bias', 'final_trans.weight', 'final_trans.bias', 'input.weight', 'input.bias', 'output.weight', 'output.bias', 'encoder_layer.self_attn.in_proj_weight', 'encoder_layer.self_attn.in_proj_bias', 'encoder_layer.self_attn.out_proj.weight', 'encoder_layer.self_attn.out_proj.bias', 'encoder_layer.linear1.weight', 'encoder_layer.linear1.bias', 'encoder_layer.linear2.weight', 'encoder_layer.linear2.bias', 'encoder_layer.norm1.weight', 'encoder_layer.norm1.bias', 'encoder_layer.norm2.weight', 'encoder_layer.norm2.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias', 'transformer_encoder.norm.weight', 'transformer_encoder.norm.bias'])
Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  24%|██▍       | 6936/28639 [00:00<00:00, 69331.30formulae/s]Generating EDM:  50%|████▉     | 14218/28639 [00:00<00:00, 71377.26formulae/s]Generating EDM:  85%|████████▍ | 24243/28639 [00:00<00:00, 84556.35formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 83420.90formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 118653.65formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
nohup: ignoring input
Property "jarvis_bandgap" selected for training
model_name:  jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_lowerLr_finetune

Model architecture: out_dims, d_model, N, heads
256, 512, 3, 4
Running on compute device: cuda:0
Model size: 21917702 parameters

<class 'collections.OrderedDict'>
loaded modules: dict_keys(['cls_token', 'encoder.emb_scaler', 'encoder.pos_scaler', 'encoder.pos_scaler_log', 'encoder.embed.fc_mat2vec.weight', 'encoder.embed.fc_mat2vec.bias', 'encoder.embed.cbfv.weight', 'encoder.pe.pe', 'encoder.ple.pe', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.0.linear1.weight', 'encoder.transformer_encoder.layers.0.linear1.bias', 'encoder.transformer_encoder.layers.0.linear2.weight', 'encoder.transformer_encoder.layers.0.linear2.bias', 'encoder.transformer_encoder.layers.0.norm1.weight', 'encoder.transformer_encoder.layers.0.norm1.bias', 'encoder.transformer_encoder.layers.0.norm2.weight', 'encoder.transformer_encoder.layers.0.norm2.bias', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.1.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.1.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.1.linear1.weight', 'encoder.transformer_encoder.layers.1.linear1.bias', 'encoder.transformer_encoder.layers.1.linear2.weight', 'encoder.transformer_encoder.layers.1.linear2.bias', 'encoder.transformer_encoder.layers.1.norm1.weight', 'encoder.transformer_encoder.layers.1.norm1.bias', 'encoder.transformer_encoder.layers.1.norm2.weight', 'encoder.transformer_encoder.layers.1.norm2.bias', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_weight', 'encoder.transformer_encoder.layers.2.self_attn.in_proj_bias', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.weight', 'encoder.transformer_encoder.layers.2.self_attn.out_proj.bias', 'encoder.transformer_encoder.layers.2.linear1.weight', 'encoder.transformer_encoder.layers.2.linear1.bias', 'encoder.transformer_encoder.layers.2.linear2.weight', 'encoder.transformer_encoder.layers.2.linear2.bias', 'encoder.transformer_encoder.layers.2.norm1.weight', 'encoder.transformer_encoder.layers.2.norm1.bias', 'encoder.transformer_encoder.layers.2.norm2.weight', 'encoder.transformer_encoder.layers.2.norm2.bias', 'output_nn.fcs.0.weight', 'output_nn.fcs.0.bias', 'output_nn.fcs.1.weight', 'output_nn.fcs.1.bias', 'output_nn.fcs.2.weight', 'output_nn.fcs.2.bias', 'output_nn.res_fcs.0.weight', 'output_nn.res_fcs.1.weight', 'output_nn.res_fcs.2.weight', 'output_nn.fc_out.weight', 'output_nn.fc_out.bias', 'res_nn.fcs.0.weight', 'res_nn.fcs.0.bias', 'res_nn.fcs.1.weight', 'res_nn.fcs.1.bias', 'res_nn.fcs.2.weight', 'res_nn.fcs.2.bias', 'res_nn.fc_out.weight', 'res_nn.fc_out.bias', 'src_nn.weight', 'src_nn.bias', 'final.weight', 'final.bias', 'final_trans.weight', 'final_trans.bias', 'input.weight', 'input.bias', 'output.weight', 'output.bias', 'encoder_layer.self_attn.in_proj_weight', 'encoder_layer.self_attn.in_proj_bias', 'encoder_layer.self_attn.out_proj.weight', 'encoder_layer.self_attn.out_proj.bias', 'encoder_layer.linear1.weight', 'encoder_layer.linear1.bias', 'encoder_layer.linear2.weight', 'encoder_layer.linear2.bias', 'encoder_layer.norm1.weight', 'encoder_layer.norm1.bias', 'encoder_layer.norm2.weight', 'encoder_layer.norm2.bias', 'transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias', 'transformer_encoder.norm.weight', 'transformer_encoder.norm.bias'])
Generating EDM:   0%|          | 0/28639 [00:00<?, ?formulae/s]Generating EDM:  28%|██▊       | 8003/28639 [00:00<00:00, 80019.96formulae/s]Generating EDM:  63%|██████▎   | 18120/28639 [00:00<00:00, 92456.42formulae/s]Generating EDM:  99%|█████████▉| 28330/28639 [00:00<00:00, 96856.85formulae/s]Generating EDM: 100%|██████████| 28639/28639 [00:00<00:00, 94460.19formulae/s]
loading data with up to 7 elements in the formula
training with batchsize 256 (2**8.000)
Generating EDM:   0%|          | 0/3185 [00:00<?, ?formulae/s]Generating EDM: 100%|██████████| 3185/3185 [00:00<00:00, 143164.88formulae/s]
loading data with up to 7 elements in the formula
stepping every 112 training passes, cycling lr every 1 epochs
checkin at 2 epochs to match lr scheduler
Epoch: 0/400 --- train mae: 0.831 val mae: 0.809
Epoch: 1/400 --- train mae: 0.72 val mae: 0.698
Epoch: 3/400 --- train mae: 0.634 val mae: 0.611
Epoch: 5/400 --- train mae: 0.608 val mae: 0.585
Epoch: 7/400 --- train mae: 0.578 val mae: 0.553
Epoch: 9/400 --- train mae: 0.529 val mae: 0.503
Epoch: 11/400 --- train mae: 0.459 val mae: 0.437
Epoch: 13/400 --- train mae: 0.406 val mae: 0.397
Epoch: 15/400 --- train mae: 0.394 val mae: 0.385
Epoch: 17/400 --- train mae: 0.382 val mae: 0.378
Epoch: 19/400 --- train mae: 0.374 val mae: 0.369
Epoch: 21/400 --- train mae: 0.37 val mae: 0.367
Epoch: 23/400 --- train mae: 0.361 val mae: 0.36
Epoch: 25/400 --- train mae: 0.354 val mae: 0.353
Epoch: 27/400 --- train mae: 0.352 val mae: 0.354
Epoch: 29/400 --- train mae: 0.345 val mae: 0.343
Epoch: 31/400 --- train mae: 0.342 val mae: 0.341
Epoch: 33/400 --- train mae: 0.339 val mae: 0.336
Epoch: 35/400 --- train mae: 0.333 val mae: 0.334
Epoch: 37/400 --- train mae: 0.331 val mae: 0.332
Epoch: 39/400 --- train mae: 0.327 val mae: 0.329
Epoch: 41/400 --- train mae: 0.325 val mae: 0.323
Epoch: 43/400 --- train mae: 0.32 val mae: 0.321
Epoch 45 failed to improve.
Discarded: 1/150 weight updates ♻🗑️
Epoch: 45/400 --- train mae: 0.321 val mae: 0.322
Epoch: 47/400 --- train mae: 0.316 val mae: 0.318
Epoch: 49/400 --- train mae: 0.313 val mae: 0.316
Epoch: 51/400 --- train mae: 0.311 val mae: 0.315
Epoch: 53/400 --- train mae: 0.305 val mae: 0.311
Epoch: 55/400 --- train mae: 0.302 val mae: 0.304
Epoch: 57/400 --- train mae: 0.301 val mae: 0.3
Epoch: 59/400 --- train mae: 0.3 val mae: 0.304
Epoch: 61/400 --- train mae: 0.297 val mae: 0.292
Epoch: 63/400 --- train mae: 0.293 val mae: 0.303
Epoch 65 failed to improve.
Discarded: 2/150 weight updates ♻🗑️
Epoch: 65/400 --- train mae: 0.291 val mae: 0.296
Epoch 67 failed to improve.
Discarded: 3/150 weight updates ♻🗑️
Epoch: 67/400 --- train mae: 0.29 val mae: 0.296
Epoch: 69/400 --- train mae: 0.287 val mae: 0.292
Epoch: 71/400 --- train mae: 0.285 val mae: 0.297
Epoch: 73/400 --- train mae: 0.281 val mae: 0.291
Epoch 75 failed to improve.
Discarded: 4/150 weight updates ♻🗑️
Epoch: 75/400 --- train mae: 0.279 val mae: 0.292
Epoch 77 failed to improve.
Discarded: 5/150 weight updates ♻🗑️
Epoch: 77/400 --- train mae: 0.277 val mae: 0.29
Epoch: 79/400 --- train mae: 0.273 val mae: 0.283
Epoch: 81/400 --- train mae: 0.275 val mae: 0.287
Epoch: 83/400 --- train mae: 0.274 val mae: 0.285
Epoch: 85/400 --- train mae: 0.269 val mae: 0.278
Epoch: 87/400 --- train mae: 0.269 val mae: 0.281
Epoch: 89/400 --- train mae: 0.265 val mae: 0.273
Epoch: 91/400 --- train mae: 0.263 val mae: 0.28
Epoch: 93/400 --- train mae: 0.261 val mae: 0.274
Epoch: 95/400 --- train mae: 0.259 val mae: 0.274
Epoch: 97/400 --- train mae: 0.26 val mae: 0.277
Epoch: 99/400 --- train mae: 0.253 val mae: 0.269
Epoch: 101/400 --- train mae: 0.254 val mae: 0.266
Epoch: 103/400 --- train mae: 0.255 val mae: 0.267
Epoch: 105/400 --- train mae: 0.25 val mae: 0.269
Epoch: 107/400 --- train mae: 0.25 val mae: 0.269
Epoch 109 failed to improve.
Discarded: 6/150 weight updates ♻🗑️
Epoch: 109/400 --- train mae: 0.248 val mae: 0.271
Epoch 111 failed to improve.
Discarded: 7/150 weight updates ♻🗑️
Epoch: 111/400 --- train mae: 0.249 val mae: 0.275
Epoch: 113/400 --- train mae: 0.248 val mae: 0.272
Epoch: 115/400 --- train mae: 0.245 val mae: 0.271
Epoch: 117/400 --- train mae: 0.24 val mae: 0.266
Epoch: 119/400 --- train mae: 0.24 val mae: 0.262
Epoch 121 failed to improve.
Discarded: 8/150 weight updates ♻🗑️
Epoch: 121/400 --- train mae: 0.24 val mae: 0.266
Epoch: 123/400 --- train mae: 0.241 val mae: 0.263
Epoch: 125/400 --- train mae: 0.239 val mae: 0.268
Epoch: 127/400 --- train mae: 0.237 val mae: 0.265
Epoch: 129/400 --- train mae: 0.235 val mae: 0.265
Epoch 131 failed to improve.
Discarded: 9/150 weight updates ♻🗑️
Epoch: 131/400 --- train mae: 0.233 val mae: 0.263
Epoch: 133/400 --- train mae: 0.231 val mae: 0.262
Epoch: 135/400 --- train mae: 0.233 val mae: 0.259
Epoch 137 failed to improve.
Discarded: 10/150 weight updates ♻🗑️
Epoch: 137/400 --- train mae: 0.23 val mae: 0.264
Epoch: 139/400 --- train mae: 0.234 val mae: 0.265
Epoch: 141/400 --- train mae: 0.227 val mae: 0.257
Epoch: 143/400 --- train mae: 0.227 val mae: 0.259
Epoch 145 failed to improve.
Discarded: 11/150 weight updates ♻🗑️
Epoch: 145/400 --- train mae: 0.226 val mae: 0.261
Epoch: 147/400 --- train mae: 0.229 val mae: 0.262
Epoch: 149/400 --- train mae: 0.223 val mae: 0.254
Epoch: 151/400 --- train mae: 0.222 val mae: 0.256
Epoch 153 failed to improve.
Discarded: 12/150 weight updates ♻🗑️
Epoch: 153/400 --- train mae: 0.224 val mae: 0.261
Epoch: 155/400 --- train mae: 0.223 val mae: 0.254
Epoch 157 failed to improve.
Discarded: 13/150 weight updates ♻🗑️
Epoch: 157/400 --- train mae: 0.22 val mae: 0.258
Epoch: 159/400 --- train mae: 0.216 val mae: 0.253
Epoch: 161/400 --- train mae: 0.218 val mae: 0.253
Epoch: 163/400 --- train mae: 0.218 val mae: 0.252
Epoch: 165/400 --- train mae: 0.213 val mae: 0.252
Epoch 167 failed to improve.
Discarded: 14/150 weight updates ♻🗑️
Epoch: 167/400 --- train mae: 0.216 val mae: 0.253
Epoch: 169/400 --- train mae: 0.212 val mae: 0.251
Epoch 171 failed to improve.
Discarded: 15/150 weight updates ♻🗑️
Epoch: 171/400 --- train mae: 0.211 val mae: 0.253
Epoch: 173/400 --- train mae: 0.21 val mae: 0.249
Epoch: 175/400 --- train mae: 0.206 val mae: 0.246
Epoch: 177/400 --- train mae: 0.208 val mae: 0.244
Epoch: 179/400 --- train mae: 0.205 val mae: 0.242
Epoch 181 failed to improve.
Discarded: 16/150 weight updates ♻🗑️
Epoch: 181/400 --- train mae: 0.208 val mae: 0.247
Epoch: 183/400 --- train mae: 0.205 val mae: 0.245
Epoch: 185/400 --- train mae: 0.206 val mae: 0.241
Epoch: 187/400 --- train mae: 0.207 val mae: 0.243
Epoch 189 failed to improve.
Discarded: 17/150 weight updates ♻🗑️
Epoch: 189/400 --- train mae: 0.205 val mae: 0.246
Epoch 191 failed to improve.
Discarded: 18/150 weight updates ♻🗑️
Epoch: 191/400 --- train mae: 0.202 val mae: 0.241
Epoch: 193/400 --- train mae: 0.2 val mae: 0.239
Epoch 195 failed to improve.
Discarded: 19/150 weight updates ♻🗑️
Epoch: 195/400 --- train mae: 0.201 val mae: 0.238
Epoch 197 failed to improve.
Discarded: 20/150 weight updates ♻🗑️
Epoch: 197/400 --- train mae: 0.203 val mae: 0.241
Epoch: 199/400 --- train mae: 0.201 val mae: 0.242
Epoch 201 failed to improve.
Discarded: 21/150 weight updates ♻🗑️
Epoch: 201/400 --- train mae: 0.199 val mae: 0.241
Epoch: 203/400 --- train mae: 0.197 val mae: 0.233
Epoch: 205/400 --- train mae: 0.194 val mae: 0.237
Epoch: 207/400 --- train mae: 0.194 val mae: 0.236
Epoch 209 failed to improve.
Discarded: 22/150 weight updates ♻🗑️
Epoch: 209/400 --- train mae: 0.196 val mae: 0.241
Epoch 211 failed to improve.
Discarded: 23/150 weight updates ♻🗑️
Epoch: 211/400 --- train mae: 0.193 val mae: 0.237
Epoch 213 failed to improve.
Discarded: 24/150 weight updates ♻🗑️
Epoch: 213/400 --- train mae: 0.19 val mae: 0.234
Epoch 215 failed to improve.
Discarded: 25/150 weight updates ♻🗑️
Epoch: 215/400 --- train mae: 0.192 val mae: 0.238
Epoch: 217/400 --- train mae: 0.193 val mae: 0.242
Epoch 219 failed to improve.
Discarded: 26/150 weight updates ♻🗑️
Epoch: 219/400 --- train mae: 0.19 val mae: 0.237
Epoch 221 failed to improve.
Discarded: 27/150 weight updates ♻🗑️
Epoch: 221/400 --- train mae: 0.191 val mae: 0.238
Epoch 223 failed to improve.
Discarded: 28/150 weight updates ♻🗑️
Epoch: 223/400 --- train mae: 0.188 val mae: 0.234
Epoch 225 failed to improve.
Discarded: 29/150 weight updates ♻🗑️
Epoch: 225/400 --- train mae: 0.191 val mae: 0.24
Epoch 227 failed to improve.
Discarded: 30/150 weight updates ♻🗑️
Epoch: 227/400 --- train mae: 0.186 val mae: 0.239
Epoch 229 failed to improve.
Discarded: 31/150 weight updates ♻🗑️
Epoch: 229/400 --- train mae: 0.185 val mae: 0.241
Epoch 231 failed to improve.
Discarded: 32/150 weight updates ♻🗑️
Epoch: 231/400 --- train mae: 0.186 val mae: 0.244
Epoch 233 failed to improve.
Discarded: 33/150 weight updates ♻🗑️
Epoch: 233/400 --- train mae: 0.185 val mae: 0.24
Epoch 235 failed to improve.
Discarded: 34/150 weight updates ♻🗑️
Epoch: 235/400 --- train mae: 0.183 val mae: 0.236
Epoch 237 failed to improve.
Discarded: 35/150 weight updates ♻🗑️
Epoch: 237/400 --- train mae: 0.182 val mae: 0.242
Epoch 239 failed to improve.
Discarded: 36/150 weight updates ♻🗑️
Epoch: 239/400 --- train mae: 0.181 val mae: 0.232
Epoch 241 failed to improve.
Discarded: 37/150 weight updates ♻🗑️
Epoch: 241/400 --- train mae: 0.182 val mae: 0.237
Epoch: 243/400 --- train mae: 0.179 val mae: 0.236
Epoch 245 failed to improve.
Discarded: 38/150 weight updates ♻🗑️
Epoch: 245/400 --- train mae: 0.179 val mae: 0.241
Epoch: 247/400 --- train mae: 0.181 val mae: 0.235
Epoch: 249/400 --- train mae: 0.179 val mae: 0.232
Epoch: 251/400 --- train mae: 0.179 val mae: 0.236
Epoch 253 failed to improve.
Discarded: 39/150 weight updates ♻🗑️
Epoch: 253/400 --- train mae: 0.175 val mae: 0.238
Epoch 255 failed to improve.
Discarded: 40/150 weight updates ♻🗑️
Epoch: 255/400 --- train mae: 0.177 val mae: 0.236
Epoch 257 failed to improve.
Discarded: 41/150 weight updates ♻🗑️
Epoch: 257/400 --- train mae: 0.175 val mae: 0.234
Epoch 259 failed to improve.
Discarded: 42/150 weight updates ♻🗑️
Epoch: 259/400 --- train mae: 0.177 val mae: 0.238
Epoch: 261/400 --- train mae: 0.173 val mae: 0.232
Epoch: 263/400 --- train mae: 0.174 val mae: 0.236
Epoch: 265/400 --- train mae: 0.176 val mae: 0.235
Epoch: 267/400 --- train mae: 0.175 val mae: 0.231
Epoch: 269/400 --- train mae: 0.173 val mae: 0.23
Epoch 271 failed to improve.
Discarded: 43/150 weight updates ♻🗑️
Epoch: 271/400 --- train mae: 0.174 val mae: 0.232
Epoch 273 failed to improve.
Discarded: 44/150 weight updates ♻🗑️
Epoch: 273/400 --- train mae: 0.169 val mae: 0.228
Epoch: 275/400 --- train mae: 0.167 val mae: 0.23
Epoch 277 failed to improve.
Discarded: 45/150 weight updates ♻🗑️
Epoch: 277/400 --- train mae: 0.171 val mae: 0.232
Epoch 279 failed to improve.
Discarded: 46/150 weight updates ♻🗑️
Epoch: 279/400 --- train mae: 0.168 val mae: 0.234
Epoch 281 failed to improve.
Discarded: 47/150 weight updates ♻🗑️
Epoch: 281/400 --- train mae: 0.167 val mae: 0.231
Epoch 283 failed to improve.
Discarded: 48/150 weight updates ♻🗑️
Epoch: 283/400 --- train mae: 0.168 val mae: 0.232
Epoch 285 failed to improve.
Discarded: 49/150 weight updates ♻🗑️
Epoch: 285/400 --- train mae: 0.168 val mae: 0.23
Epoch 287 failed to improve.
Discarded: 50/150 weight updates ♻🗑️
Epoch: 287/400 --- train mae: 0.166 val mae: 0.23
Epoch: 289/400 --- train mae: 0.167 val mae: 0.23
Epoch 291 failed to improve.
Discarded: 51/150 weight updates ♻🗑️
Epoch: 291/400 --- train mae: 0.166 val mae: 0.232
Epoch: 293/400 --- train mae: 0.168 val mae: 0.227
Epoch: 295/400 --- train mae: 0.166 val mae: 0.232
Epoch: 297/400 --- train mae: 0.164 val mae: 0.229
Epoch: 299/400 --- train mae: 0.163 val mae: 0.225
Epoch: 301/400 --- train mae: 0.162 val mae: 0.23
Epoch 303 failed to improve.
Discarded: 52/150 weight updates ♻🗑️
Epoch: 303/400 --- train mae: 0.163 val mae: 0.233
Epoch 305 failed to improve.
Discarded: 53/150 weight updates ♻🗑️
Epoch: 305/400 --- train mae: 0.161 val mae: 0.233
Epoch 307 failed to improve.
Discarded: 54/150 weight updates ♻🗑️
Epoch: 307/400 --- train mae: 0.161 val mae: 0.233
Epoch 309 failed to improve.
Discarded: 55/150 weight updates ♻🗑️
Epoch: 309/400 --- train mae: 0.16 val mae: 0.236
Epoch: 311/400 --- train mae: 0.157 val mae: 0.226
Epoch: 313/400 --- train mae: 0.161 val mae: 0.23
Epoch 315 failed to improve.
Discarded: 56/150 weight updates ♻🗑️
Epoch: 315/400 --- train mae: 0.158 val mae: 0.226
Epoch 317 failed to improve.
Discarded: 57/150 weight updates ♻🗑️
Epoch: 317/400 --- train mae: 0.158 val mae: 0.23
Epoch 319 failed to improve.
Discarded: 58/150 weight updates ♻🗑️
Epoch: 319/400 --- train mae: 0.157 val mae: 0.231
Epoch: 321/400 --- train mae: 0.155 val mae: 0.226
Epoch: 323/400 --- train mae: 0.158 val mae: 0.231
Epoch 325 failed to improve.
Discarded: 59/150 weight updates ♻🗑️
Epoch: 325/400 --- train mae: 0.155 val mae: 0.228
Epoch 327 failed to improve.
Discarded: 60/150 weight updates ♻🗑️
Epoch: 327/400 --- train mae: 0.155 val mae: 0.23
Epoch 329 failed to improve.
Discarded: 61/150 weight updates ♻🗑️
Epoch: 329/400 --- train mae: 0.154 val mae: 0.227
Epoch: 331/400 --- train mae: 0.153 val mae: 0.225
Epoch 333 failed to improve.
Discarded: 62/150 weight updates ♻🗑️
Epoch: 333/400 --- train mae: 0.149 val mae: 0.227
Epoch 335 failed to improve.
Discarded: 63/150 weight updates ♻🗑️
Epoch: 335/400 --- train mae: 0.152 val mae: 0.228
Epoch: 337/400 --- train mae: 0.155 val mae: 0.228
Epoch: 339/400 --- train mae: 0.151 val mae: 0.223
Epoch 341 failed to improve.
Discarded: 64/150 weight updates ♻🗑️
Epoch: 341/400 --- train mae: 0.153 val mae: 0.232
Epoch 343 failed to improve.
Discarded: 65/150 weight updates ♻🗑️
Epoch: 343/400 --- train mae: 0.149 val mae: 0.23
Epoch 345 failed to improve.
Discarded: 66/150 weight updates ♻🗑️
Epoch: 345/400 --- train mae: 0.149 val mae: 0.228
Epoch 347 failed to improve.
Discarded: 67/150 weight updates ♻🗑️
Epoch: 347/400 --- train mae: 0.149 val mae: 0.228
Epoch 349 failed to improve.
Discarded: 68/150 weight updates ♻🗑️
Epoch: 349/400 --- train mae: 0.148 val mae: 0.226
Epoch 351 failed to improve.
Discarded: 69/150 weight updates ♻🗑️
Epoch: 351/400 --- train mae: 0.144 val mae: 0.224
Epoch 353 failed to improve.
Discarded: 70/150 weight updates ♻🗑️
Epoch: 353/400 --- train mae: 0.149 val mae: 0.229
Epoch: 355/400 --- train mae: 0.147 val mae: 0.223
Epoch: 357/400 --- train mae: 0.146 val mae: 0.228
Epoch 359 failed to improve.
Discarded: 71/150 weight updates ♻🗑️
Epoch: 359/400 --- train mae: 0.145 val mae: 0.226
Epoch 361 failed to improve.
Discarded: 72/150 weight updates ♻🗑️
Epoch: 361/400 --- train mae: 0.144 val mae: 0.227
Epoch 363 failed to improve.
Discarded: 73/150 weight updates ♻🗑️
Epoch: 363/400 --- train mae: 0.144 val mae: 0.23
Epoch: 365/400 --- train mae: 0.143 val mae: 0.225
Epoch: 367/400 --- train mae: 0.14 val mae: 0.218
Epoch: 369/400 --- train mae: 0.141 val mae: 0.218
Epoch: 371/400 --- train mae: 0.143 val mae: 0.226
Epoch 373 failed to improve.
Discarded: 74/150 weight updates ♻🗑️
Epoch: 373/400 --- train mae: 0.142 val mae: 0.226
Epoch 375 failed to improve.
Discarded: 75/150 weight updates ♻🗑️
Epoch: 375/400 --- train mae: 0.14 val mae: 0.224
Epoch: 377/400 --- train mae: 0.139 val mae: 0.22
Epoch 379 failed to improve.
Discarded: 76/150 weight updates ♻🗑️
Epoch: 379/400 --- train mae: 0.14 val mae: 0.226
Epoch 381 failed to improve.
Discarded: 77/150 weight updates ♻🗑️
Epoch: 381/400 --- train mae: 0.138 val mae: 0.221
Epoch 383 failed to improve.
Discarded: 78/150 weight updates ♻🗑️
Epoch: 383/400 --- train mae: 0.139 val mae: 0.221
Epoch 385 failed to improve.
Discarded: 79/150 weight updates ♻🗑️
Epoch: 385/400 --- train mae: 0.138 val mae: 0.221
Epoch: 387/400 --- train mae: 0.14 val mae: 0.223
Epoch 389 failed to improve.
Discarded: 80/150 weight updates ♻🗑️
Epoch: 389/400 --- train mae: 0.141 val mae: 0.225
Epoch 391 failed to improve.
Discarded: 81/150 weight updates ♻🗑️
Epoch: 391/400 --- train mae: 0.135 val mae: 0.219
Epoch: 393/400 --- train mae: 0.139 val mae: 0.225
Epoch: 395/400 --- train mae: 0.133 val mae: 0.217
Epoch 397 failed to improve.
Discarded: 82/150 weight updates ♻🗑️
Epoch: 397/400 --- train mae: 0.133 val mae: 0.218
Epoch 399 failed to improve.
Discarded: 83/150 weight updates ♻🗑️
Epoch: 399/400 --- train mae: 0.132 val mae: 0.217
Saving network (jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_lowerLr_finetune) to /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer/jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_lowerLr_finetune.pth
=====================================================
                    jarvis_bandgap                   
=====================================================
calculating train mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_lowerLr_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.132
-----------------------------------------------------
calculating val mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_lowerLr_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.212
-----------------------------------------------------
calculating test mae
jarvis_bandgap_pretrain_transformer_en_de_Linear_clstoken_adjust_layerNorm_lowerLr_finetune /home/zd/zd/teaching_net/CrabNet/models/jarvis_bandgap_models/pretrained_transformer
loading data with up to 7 elements in the formula
jarvis_bandgap mae: 0.226
=====================================================
